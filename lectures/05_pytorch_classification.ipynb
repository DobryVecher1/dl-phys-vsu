{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPPr3hD/e8EKBRLlCH/8yPK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DobryVecher1/dl-phys-vsu/blob/main/lectures/05_pytorch_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "ChTc2Bmq4aIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torchmetrics"
      ],
      "metadata": {
        "id": "3pfcsyrDnlYn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "# cls_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck',]"
      ],
      "metadata": {
        "id": "O3llllC9nsGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train.shape"
      ],
      "metadata": {
        "id": "refGcIaboYm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetCIFAR(Dataset):\n",
        "\n",
        "    def __init__(self, x_data, y_data, transform=None):\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Load and return a sample from the dataset at the given index.\"\"\"\n",
        "        img = self.x_data[index]\n",
        "\n",
        "        # augmentations\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = torch.from_numpy(self.y_data[index])\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of samples in dataset.\"\"\"\n",
        "        return len(self.x_data)"
      ],
      "metadata": {
        "id": "s6Hwyb51n7gH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatamoduleCIFAR():\n",
        "    \"\"\"Create dataset and loaders, apply transforms.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # load data\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "        # make dataset smaller if needed\n",
        "        # self.x_train = self.x_train[:1000]\n",
        "        # self.y_train = self.y_train[:1000]\n",
        "        # self.x_test = self.x_test[:1000]\n",
        "        # self.y_test = self.y_test[:1000]\n",
        "\n",
        "    def create_loaders(self):\n",
        "        \"\"\"Create loaders both for train and test/validation datasets.\"\"\"\n",
        "\n",
        "        # train dataset\n",
        "        dset_train = DatasetCIFAR(self.x_train, self.y_train, transform=transforms.ToTensor())\n",
        "        # test dataset\n",
        "        dset_test = DatasetCIFAR(self.x_test, self.y_test, transform=transforms.ToTensor())\n",
        "\n",
        "        # Train and test dataloaders\n",
        "        train_loader = DataLoader(dset_train, batch_size=100, shuffle=True)\n",
        "        test_loader = DataLoader(dset_test, batch_size=100, shuffle=False)\n",
        "\n",
        "        return train_loader, test_loader"
      ],
      "metadata": {
        "id": "7_706Y1Or2no"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelCIFAR(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=4 * 4 * 16, out_features=10)\n",
        "        )\n",
        "\n",
        "        self.loss_ce = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.AdamW(self.cnn.parameters(), lr=1e-3)\n",
        "\n",
        "        # Metrics\n",
        "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
        "        self.prec = torchmetrics.Precision(task='multiclass', num_classes=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cnn(x)\n",
        "\n",
        "    def fit(self, train_loader, test_loader, num_epoch=50):\n",
        "\n",
        "        for ii in range(num_epoch):\n",
        "\n",
        "            loss_batches = []\n",
        "            preds_train = []\n",
        "            labels_train = []\n",
        "            # train\n",
        "            for step, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "                # to cuda\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                self.cnn.train()\n",
        "                # make prediction\n",
        "                logits_cls = self.forward(images)\n",
        "                # calculate loss\n",
        "                loss = self.loss_ce(logits_cls, labels[:, 0])\n",
        "\n",
        "                # update weights\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # save loss\n",
        "                loss_batches.append(loss.item())\n",
        "\n",
        "                # predictions\n",
        "                labels_pred = torch.argmax(nn.Softmax(dim=1)(logits_cls), dim=1)\n",
        "\n",
        "                preds_train.append(labels_pred)\n",
        "                labels_train.append(labels[:, 0])\n",
        "\n",
        "\n",
        "            # find metrics in the end of the epoch\n",
        "            predictions = torch.cat([preds for preds in preds_train])\n",
        "            labels = torch.cat([labels for labels in labels_train])\n",
        "\n",
        "            acc_train = self.accuracy(predictions, labels)\n",
        "            prec_train = self.prec(predictions, labels)\n",
        "\n",
        "            print(f\"Epoch: {ii}\")\n",
        "            print(f\"TRAIN | Loss: {np.mean(loss_batches): .3f}, Train_acc: {acc_train: .3f}, Train_prec: {prec_train: .3f}\")\n",
        "\n",
        "            # test\n",
        "            with torch.no_grad():\n",
        "                loss_batches_test = []\n",
        "                preds_test = []\n",
        "                labels_test = []\n",
        "                for step, (images, labels) in enumerate(test_loader):\n",
        "\n",
        "                    images = images.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                    self.cnn.eval()\n",
        "                    logits_cls = self.forward(images)\n",
        "\n",
        "                    loss = self.loss_ce(logits_cls, labels[:, 0])\n",
        "\n",
        "                    # save loss\n",
        "                    loss_batches_test.append(loss.item())\n",
        "\n",
        "                    # predictions\n",
        "                    labels_pred = torch.argmax(nn.Softmax(dim=1)(logits_cls), dim=1)\n",
        "\n",
        "                    preds_test.append(labels_pred)\n",
        "                    labels_test.append(labels[:, 0])\n",
        "\n",
        "                # find metrics in the end of the epoch\n",
        "                predictions = torch.cat([preds for preds in preds_test])\n",
        "                labels = torch.cat([labels for labels in labels_test])\n",
        "\n",
        "                acc_test = self.accuracy(predictions, labels)\n",
        "                prec_test = self.prec(predictions, labels)\n",
        "\n",
        "                print(f\"TEST | Loss: {np.mean(loss_batches_test): .3f}, Test_acc: {acc_test: .3f}, Test_prec: {prec_test: .3f}\")\n"
      ],
      "metadata": {
        "id": "GjBKBe1Fva0c"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = ModelCIFAR().cuda()\n",
        "\n",
        "train_loader, test_loader = DatamoduleCIFAR().create_loaders()\n",
        "\n",
        "cnn_model.fit(train_loader, test_loader)"
      ],
      "metadata": {
        "id": "eRj4BzF64OPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJVhYujq4XU3"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}